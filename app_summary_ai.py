import streamlit as st  # type: ignore
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity
from transformers import T5Tokenizer, T5ForConditionalGeneration
import time

# Thi·∫øt l·∫≠p trang
st.set_page_config(
    page_title="AI Text Summarizer",
    page_icon="üìù",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS t√πy ch·ªânh
st.markdown("""
    <style>
    .main {
        padding: 1rem;
    }
    .stTextArea textarea {
        font-size: 14px;
    }
    .css-1d391kg {
        padding: 1rem 0.5rem;
    }
    .stProgress .st-bo {
        background-color: #4CAF50;
    }
    .stMarkdown {
        font-size: 14px;
    }
    .stButton button {
        padding: 0.5rem 2rem;
        width: auto;
        min-width: 120px;
        margin: 1rem auto;
        display: block;
        border-radius: 20px;
        font-weight: 500;
        transition: all 0.3s ease;
    }
    .stButton button:hover {
        transform: translateY(-2px);
        box-shadow: 0 2px 5px rgba(0,0,0,0.2);
    }
    div[data-testid="stSidebar"] {
        padding: 1rem 0.5rem;
    }
    div[data-testid="stButton"] {
        text-align: center;
        width: 100%;
        display: flex;
        justify-content: center;
    }
    div[data-testid="stButton"] > button {
        margin: 1rem auto;
    }
    </style>
    """, unsafe_allow_html=True)

# T·∫£i punkt
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Danh s√°ch c√°c t·ª´ n·ªëi c·∫ßn lo·∫°i b·ªè
stopwords = [
    "tr∆∞·ªõc khi", "sau khi", "trong khi", "khi m√†", "l√∫c ƒë√≥", "b√¢y gi·ªù", "hi·ªán t·∫°i", "l√∫c n√†y", "ngay l·∫≠p t·ª©c",
    "b·ªüi v√¨", "v√¨", "do", "b·ªüi", "k·∫øt qu·∫£ l√†", "d·∫´n ƒë·∫øn", "v√¨ l·∫Ω ƒë√≥", "ch√≠nh v√¨ th·∫ø", "t·ª´ ƒë√≥",
    "nh∆∞ng", "nh∆∞ng m√†", "tuy", "m·∫∑c d√π v·∫≠y", "tr√°i l·∫°i", "ng∆∞·ª£c l·∫°i", "tuy v·∫≠y", "d√π v·∫≠y", "tuy th·∫ø",
    "ngo√†i ra", "th√™m v√†o ƒë√≥", "h∆°n n·ªØa", "kh√¥ng nh·ªØng th·∫ø", "b√™n c·∫°nh ƒë√≥", "c≈©ng nh∆∞", "ƒë·ªìng th·ªùi",
    "n·∫øu", "n·∫øu nh∆∞", "gi·∫£ s·ª≠", "v·ªõi ƒëi·ªÅu ki·ªán", "trong tr∆∞·ªùng h·ª£p", "mi·ªÖn l√†", "cho d√π", "d√π cho",
    "ƒë·ªÉ", "nh·∫±m", "v·ªõi m·ª•c ƒë√≠ch", "nh·∫±m m·ª•c ƒë√≠ch", "h·∫ßu", "ƒë·ªÉ m√†", "nh·∫±m m·ª•c ti√™u",
    "gi·ªëng nh∆∞", "t∆∞∆°ng t·ª±", "c≈©ng gi·ªëng", "kh√°c v·ªõi", "so v·ªõi", "h∆°n", "k√©m", "nh∆∞ l√†",
    "hay l√†", "sau ƒë√≥", "v√¨ v·∫≠y", "cho n√™n", "tuy nhi√™n", "m·∫∑c d√π", "do ƒë√≥", "v√¨ th·∫ø", "th·∫ø nh∆∞ng",
    "b√™n c·∫°nh ƒë√≥", "ngo√†i ra", "ƒë·ªìng th·ªùi", "trong khi ƒë√≥", "h∆°n n·ªØa", "c√≥ th·ªÉ n√≥i",
    "m·∫∑t kh√°c", "t·ª´ ƒë√≥", "tr√™n th·ª±c t·∫ø", "k·∫øt lu·∫≠n l·∫°i", "c·ª• th·ªÉ l√†", "n√≥i chung",
    "nh√¨n chung", "n√≥i c√°ch kh√°c", "t√≥m l·∫°i", "t·ª´ tr∆∞·ªõc ƒë·∫øn nay", "v·ªÅ c∆° b·∫£n", "th·ª±c ra",
    "ƒëi·ªÅu ƒë√≥ cho th·∫•y", "nh∆∞ v·∫≠y", "k·∫øt qu·∫£ l√†", "suy ra", "n√≥i t√≥m l·∫°i"
]

# --- Ti·ªÅn x·ª≠ l√Ω ---
def tokenize_sentences(text):
    return nltk.sent_tokenize(text)

def remove_stopwords(sentences):
    cleaned_sentences = []
    for sentence in sentences:
        cleaned_sentence = sentence
        for stopword in stopwords:
            cleaned_sentence = cleaned_sentence.replace(stopword, "")
        cleaned_sentences.append(cleaned_sentence)
    return cleaned_sentences

def encode_sentences(sentences):
    vectorizer = TfidfVectorizer()
    return vectorizer.fit_transform(sentences)

# --- Ph√¢n c·ª•m ---
def find_optimal_clusters(sentence_vectors, max_k=8):
    distortions, silhouette_scores, K_valid = [], [], []
    for k in range(2, max_k + 1):
        if k >= sentence_vectors.shape[0]:
            break
        kmeans = KMeans(n_clusters=k, random_state=0).fit(sentence_vectors)
        labels = kmeans.labels_
        if len(set(labels)) < 2: continue
        distortions.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(sentence_vectors, labels))
        K_valid.append(k)
    return K_valid, distortions, silhouette_scores

def cluster_sentences(sentence_vectors, num_clusters):
    kmeans = KMeans(n_clusters=num_clusters, random_state=0)
    kmeans.fit(sentence_vectors)
    return kmeans

def summarize_by_clustering(sentences, kmeans):
    labels = kmeans.labels_
    summary = []
    for cluster in set(labels):
        indices = [i for i, label in enumerate(labels) if label == cluster]
        summary.append(sentences[indices[0]])  # C√¢u ƒë·∫°i di·ªán m·ªói c·ª•m
    return ' '.join(summary)

# --- Heuristic Search ---
def compute_tfidf(sentences):
    vectorizer = TfidfVectorizer()
    return vectorizer.fit_transform(sentences)

def compute_cosine_similarity(tfidf_matrix):
    return cosine_similarity(tfidf_matrix, tfidf_matrix)

def heuristic_search(sentences, cosine_sim, num_summary_sentences=3):
    sentence_scores = cosine_sim.sum(axis=1)
    ranked_sentences = sentence_scores.argsort()[::-1]
    summary = [sentences[i] for i in ranked_sentences[:num_summary_sentences]]
    return ' '.join(summary)

# --- M√¥ h√¨nh h·ªçc s√¢u ---
@st.cache_resource
def load_finetuned_model(model_path="duonggbill/dbill-model-summary"):  # <-- d√πng Hugging Face model
    tokenizer = T5Tokenizer.from_pretrained(model_path)
    model = T5ForConditionalGeneration.from_pretrained(model_path)
    return model, tokenizer

def summarize_with_t5(text, model, tokenizer):
    input_ids = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(input_ids, max_length=150, min_length=80, length_penalty=2.0, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# --- T√≥m t·∫Øt k·∫øt h·ª£p ---
def hybrid_summarization(text, model, tokenizer, num_summary_sentences=3):
    sentences = tokenize_sentences(text)
    cleaned_sentences = remove_stopwords(sentences)

    if len(cleaned_sentences) < num_summary_sentences:
        return "VƒÉn b·∫£n qu√° ng·∫Øn ƒë·ªÉ t√≥m t·∫Øt."

    sentence_vectors = encode_sentences(cleaned_sentences)
    _, _, silhouette_scores = find_optimal_clusters(sentence_vectors)
    num_clusters = silhouette_scores.index(max(silhouette_scores)) + 2 if silhouette_scores else 1
    kmeans = cluster_sentences(sentence_vectors, num_clusters)
    clustered_summary = summarize_by_clustering(sentences, kmeans)

    # T√≥m t·∫Øt heuristic t·ª´ k·∫øt qu·∫£ ph√¢n c·ª•m
    heuristic_sentences = tokenize_sentences(clustered_summary)
    cleaned = remove_stopwords(heuristic_sentences)
    tfidf_matrix = compute_tfidf(cleaned)
    cosine_sim = compute_cosine_similarity(tfidf_matrix)
    selected = heuristic_search(heuristic_sentences, cosine_sim, num_summary_sentences)

    # ƒê∆∞a v√†o m√¥ h√¨nh h·ªçc s√¢u ƒë·ªÉ sinh b·∫£n t√≥m t·∫Øt cu·ªëi c√πng
    return summarize_with_t5(selected, model, tokenizer)

# --- Giao di·ªán Streamlit ---
def main():
    # Sidebar
    with st.sidebar:
        st.title("‚öôÔ∏è C√†i ƒë·∫∑t")
        st.markdown("---")
        
        # C·∫•u h√¨nh t√≥m t·∫Øt
        st.subheader("C·∫•u h√¨nh t√≥m t·∫Øt")
        num_sentences = st.slider(
            "S·ªë c√¢u trong b·∫£n t√≥m t·∫Øt",
            min_value=1,
            max_value=10,
            value=3,
            help="S·ªë l∆∞·ª£ng c√¢u b·∫°n mu·ªën trong b·∫£n t√≥m t·∫Øt"
        )
        
        # Hi·ªÉn th·ªã th√¥ng tin
        st.markdown("---")
        st.subheader("‚ÑπÔ∏è Th√¥ng tin")
        st.markdown("""
        ·ª®ng d·ª•ng s·ª≠ d·ª•ng k·∫øt h·ª£p 3 ph∆∞∆°ng ph√°p:
        - ü§ñ M√¥ h√¨nh h·ªçc s√¢u (T5)
        - üìä Ph√¢n c·ª•m (Clustering)
        - üîç T√¨m ki·∫øm heuristic
        """)

    # Main content
    st.title("üìÑ T√≥m t·∫Øt vƒÉn b·∫£n b·∫±ng AI")
    st.markdown("·ª®ng d·ª•ng n√†y gi√∫p b·∫°n t√≥m t·∫Øt vƒÉn b·∫£n m·ªôt c√°ch th√¥ng minh.")

    # Input area
    col1, col2 = st.columns([3, 2])
    with col1:
        text_input = st.text_area(
            "‚úçÔ∏è Nh·∫≠p ƒëo·∫°n vƒÉn b·∫£n c·∫ßn t√≥m t·∫Øt:",
            height=200,
            help="Nh·∫≠p ho·∫∑c d√°n vƒÉn b·∫£n c·∫ßn t√≥m t·∫Øt v√†o ƒë√¢y"
        )
        
        # Process button - centered within the input column
        if st.button("üöÄ T√≥m t·∫Øt"):
            if not text_input.strip():
                st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p ƒëo·∫°n vƒÉn b·∫£n.")
                return

            # Progress bar
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Load model
            status_text.text("üîÑ ƒêang t·∫£i m√¥ h√¨nh...")
            model, tokenizer = load_finetuned_model()
            progress_bar.progress(20)
            
            # Process text
            status_text.text("‚öôÔ∏è ƒêang x·ª≠ l√Ω vƒÉn b·∫£n...")
            time.sleep(0.5)
            progress_bar.progress(50)
            
            # Generate summary
            status_text.text("üìù ƒêang t·∫°o b·∫£n t√≥m t·∫Øt...")
            summary = hybrid_summarization(text_input, model, tokenizer, num_sentences)
            progress_bar.progress(100)
            
            # Display results
            status_text.text("‚úÖ Ho√†n th√†nh!")
            time.sleep(0.5)
            
            # Results section
            st.markdown("---")
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.subheader("üìù B·∫£n t√≥m t·∫Øt:")
                st.markdown(f"<div style='background-color: #f0f2f6; padding: 15px; border-radius: 8px; font-size: 14px;'>{summary}</div>", unsafe_allow_html=True)
            
            with col2:
                st.subheader("üìä Th·ªëng k√™:")
                original_length = len(text_input.split())
                summary_length = len(summary.split())
                compression_ratio = (1 - summary_length/original_length) * 100
                
                st.metric("ƒê·ªô d√†i vƒÉn b·∫£n g·ªëc", f"{original_length} t·ª´")
                st.metric("ƒê·ªô d√†i b·∫£n t√≥m t·∫Øt", f"{summary_length} t·ª´")
                st.metric("T·ª∑ l·ªá n√©n", f"{compression_ratio:.1f}%")
    
    with col2:
        st.markdown("### üìã H∆∞·ªõng d·∫´n")
        st.markdown("""
        1. Nh·∫≠p ho·∫∑c d√°n vƒÉn b·∫£n
        2. ƒêi·ªÅu ch·ªânh s·ªë c√¢u t√≥m t·∫Øt
        3. Nh·∫•n n√∫t 'T√≥m t·∫Øt'
        4. Xem k·∫øt qu·∫£
        """)

if __name__ == "__main__":
    main()
