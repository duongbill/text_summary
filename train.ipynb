{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu đã được lưu thành Dataset_articles_10000.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Đọc dữ liệu từ file CSV\n",
    "df = pd.read_csv(\"./data/Dataset_articles_converted.csv\")\n",
    "\n",
    "# Chọn 30,000 dòng đầu tiên\n",
    "df = df.head(10000)\n",
    "\n",
    "# Chuyển đổi DataFrame sang Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "df.to_csv(\"Dataset_articles_10000.csv\", index=False)\n",
    "print(\"Dữ liệu đã được lưu thành Dataset_articles_10000.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a8b21aeb0b45a0ab351b0b57c1b0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "model_name = \"t5-small\"  # hoặc tên model mà bạn muốn sử dụng\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Tạo input với prefix \"summarize: \"\n",
    "    inputs = [\"summarize: \" + str(text) for text in examples[\"Contents\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=False)\n",
    "    \n",
    "    # Tokenize labels (tóm tắt)\n",
    "    summaries = [str(summary) for summary in examples[\"Summary\"]]\n",
    "    labels = tokenizer(summaries, max_length=150, truncation=True, padding=False)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize dataset đã cắt (không áp dụng padding cố định ở đây)\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia dataset thành tập train và eval (ví dụ 90% train, 10% eval)\n",
    "split_dataset = tokenized_datasets.train_test_split(test_size=0.8, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5075642318240debaab5e2284101a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1629, 'grad_norm': 1.1291643381118774, 'learning_rate': 2.52e-05, 'epoch': 0.8}\n",
      "{'loss': 1.9933, 'grad_norm': 1.208406686782837, 'learning_rate': 2.04e-05, 'epoch': 1.6}\n",
      "{'loss': 1.9384, 'grad_norm': 1.3569822311401367, 'learning_rate': 1.56e-05, 'epoch': 2.4}\n",
      "{'loss': 1.9122, 'grad_norm': 1.7546330690383911, 'learning_rate': 1.08e-05, 'epoch': 3.2}\n",
      "{'loss': 1.8835, 'grad_norm': 1.1515312194824219, 'learning_rate': 6e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33530ecf163f4047af0c80135f66a8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8001431226730347, 'eval_runtime': 1337.0228, 'eval_samples_per_second': 5.983, 'eval_steps_per_second': 2.992, 'epoch': 4.0}\n",
      "{'loss': 1.8722, 'grad_norm': 1.4307829141616821, 'learning_rate': 1.2000000000000002e-06, 'epoch': 4.8}\n",
      "{'train_runtime': 7655.2713, 'train_samples_per_second': 1.306, 'train_steps_per_second': 0.163, 'train_loss': 1.9566175048828125, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=1.9566175048828125, metrics={'train_runtime': 7655.2713, 'train_samples_per_second': 1.306, 'train_steps_per_second': 0.163, 'total_flos': 1350823787298816.0, 'train_loss': 1.9566175048828125, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Load tokenizer và mô hình (ví dụ sử dụng t5-small)\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Data collator để padding động\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save_pretrained(\"./fine_tuned_t5\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_t5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_t5\\\\tokenizer_config.json',\n",
       " './fine_tuned_t5\\\\special_tokens_map.json',\n",
       " './fine_tuned_t5\\\\spiece.model',\n",
       " './fine_tuned_t5\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_t5\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_t5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tóm tắt: ây là mt on văn dài cn c tóm tt bng mô hnh  fine-tune.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load model đã fine-tune\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./fine_tuned_t5\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./fine_tuned_t5\")\n",
    "\n",
    "# Test với một văn bản mới\n",
    "def summarize(text):\n",
    "    input_ids = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids\n",
    "    output_ids = model.generate(input_ids, max_length=128, num_beams=5, early_stopping=True)\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Ví dụ tóm tắt\n",
    "text = \"Đây là một đoạn văn dài cần được tóm tắt bằng mô hình đã fine-tune.\"\n",
    "print(\"Tóm tắt:\", summarize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contents</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Những ngày vừa qua, trên trang Facebook chính ...</td>\n",
       "      <td>Lâm Đồng - Lãnh đạo thành phố Bảo Lộc, Lâm Đồn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Theo thông tin từ Cục Thuế TP.HCM, hiện cơ qua...</td>\n",
       "      <td>TPHCM - Việc không thể cưỡng chế thuế của hai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNLĐ mong muốn sớm được tiếp cận với nhà ở xã ...</td>\n",
       "      <td>Hiện trên địa bàn tỉnh Ninh Bình có 32 khu, cụ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hoàn công nhà ở là một thủ tục hành chính tron...</td>\n",
       "      <td>Hoàn công nhà ở với ý nghĩa là điều kiện để đư...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Đi dọc đường Lê Văn Lương kéo dài xuống khu Dư...</td>\n",
       "      <td>Có rất nhiều lý do khiến những dự án thấp nội ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Contents  \\\n",
       "0  Những ngày vừa qua, trên trang Facebook chính ...   \n",
       "1  Theo thông tin từ Cục Thuế TP.HCM, hiện cơ qua...   \n",
       "2  CNLĐ mong muốn sớm được tiếp cận với nhà ở xã ...   \n",
       "3  Hoàn công nhà ở là một thủ tục hành chính tron...   \n",
       "4  Đi dọc đường Lê Văn Lương kéo dài xuống khu Dư...   \n",
       "\n",
       "                                             Summary  \n",
       "0  Lâm Đồng - Lãnh đạo thành phố Bảo Lộc, Lâm Đồn...  \n",
       "1  TPHCM - Việc không thể cưỡng chế thuế của hai ...  \n",
       "2  Hiện trên địa bàn tỉnh Ninh Bình có 32 khu, cụ...  \n",
       "3  Hoàn công nhà ở với ý nghĩa là điều kiện để đư...  \n",
       "4  Có rất nhiều lý do khiến những dự án thấp nội ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
