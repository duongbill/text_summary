import streamlit as st
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

# Táº£i punkt náº¿u chÆ°a cÃ³
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Danh sÃ¡ch cÃ¡c tá»« ná»‘i cáº§n loáº¡i bá»
stopwords = ["hay lÃ ", "sau Ä‘Ã³", "vÃ¬ váº­y", "cho nÃªn", "tuy nhiÃªn", "máº·c dÃ¹", "do Ä‘Ã³", "vÃ¬ tháº¿", "tháº¿ nhÆ°ng"]

# HÃ m loáº¡i bá» cÃ¡c tá»« ná»‘i
def remove_stopwords(sentences):
    return [sentence for sentence in sentences if not any(stopword in sentence for stopword in stopwords)]

# --- TÃ³m táº¯t phÃ¢n cá»¥m ---
def tokenize_sentences(text):
    return nltk.sent_tokenize(text)

def encode_sentences(sentences):
    vectorizer = TfidfVectorizer()
    return vectorizer.fit_transform(sentences)

def find_optimal_clusters(sentence_vectors, max_k=8):
    distortions, silhouette_scores, K_valid = [], [], []

    for k in range(2, max_k + 1):
        if k >= sentence_vectors.shape[0]:
            break
        kmeans = KMeans(n_clusters=k, random_state=0).fit(sentence_vectors)
        labels = kmeans.labels_
        if len(set(labels)) < 2: continue
        distortions.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(sentence_vectors, labels))
        K_valid.append(k)

    return K_valid, distortions, silhouette_scores

def cluster_sentences(sentence_vectors, num_clusters):
    kmeans = KMeans(n_clusters=num_clusters, random_state=0)
    kmeans.fit(sentence_vectors)
    return kmeans

def summarize_by_clustering(sentences, kmeans):
    labels = kmeans.labels_
    summary = []
    for cluster in set(labels):
        indices = [i for i, label in enumerate(labels) if label == cluster]
        summary.append(sentences[indices[0]])  # CÃ¢u Ä‘áº¡i diá»‡n
    return ' '.join(summary)

# --- TÃ³m táº¯t Heuristic Search (TF-IDF + Cosine Similarity) ---
def compute_tfidf(sentences):
    vectorizer = TfidfVectorizer()
    return vectorizer.fit_transform(sentences)

def compute_cosine_similarity(tfidf_matrix):
    return cosine_similarity(tfidf_matrix, tfidf_matrix)

def heuristic_search(sentences, cosine_sim, num_summary_sentences=3):
    sentence_scores = cosine_sim.sum(axis=1)  # Tá»•ng Ä‘iá»ƒm cosine cho má»—i cÃ¢u
    ranked_sentences = sentence_scores.argsort()[::-1]  # Sáº¯p xáº¿p cÃ¢u theo Ä‘iá»ƒm sá»‘ giáº£m dáº§n
    summary = [sentences[i] for i in ranked_sentences[:num_summary_sentences]]
    return ' '.join(summary)

# --- TÃ³m táº¯t há»c sÃ¢u ---
@st.cache_resource
def load_finetuned_model(model_path="duong-ai/bart-finetuned-summary"):
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)
    return pipeline("summarization", model=model, tokenizer=tokenizer)

def summarize_with_bart(text, model):
    result = model(text, max_length=45, min_length=10, do_sample=False)
    return result[0]['summary_text']

# --- Váº½ cá»¥m ---
def plot_clusters(vectors, labels, method="pca"):
    reducer = PCA(n_components=2) if method == "pca" else TSNE(n_components=2, random_state=42)
    reduced = reducer.fit_transform(vectors.toarray())
    plt.figure(figsize=(8, 5))
    sns.scatterplot(x=reduced[:,0], y=reduced[:,1], hue=labels, palette="tab10")
    plt.title(f"Biá»ƒu Ä‘á»“ cá»¥m theo {method.upper()}")
    st.pyplot(plt.gcf())

# --- Streamlit App ---
def main():
    st.title("ðŸ“„ TÃ³m táº¯t vÄƒn báº£n báº±ng AI (TF-IDF + Há»c sÃ¢u)")
    st.write("TÃ³m táº¯t vÄƒn báº£n báº±ng KMeans vÃ  mÃ´ hÃ¬nh há»c sÃ¢u fine-tuned (BART/T5)")

    text_input = st.text_area("âœï¸ Nháº­p Ä‘oáº¡n vÄƒn báº£n cáº§n tÃ³m táº¯t:")

    if st.button("ðŸš€ TÃ³m táº¯t"):
        if not text_input.strip():
            st.warning("Vui lÃ²ng nháº­p Ä‘oáº¡n vÄƒn báº£n.")
            return

        sentences = tokenize_sentences(text_input)

        # Loáº¡i bá» cÃ¡c tá»« ná»‘i
        sentences = remove_stopwords(sentences)
        
        if len(sentences) < 3:
            st.warning("VÄƒn báº£n cáº§n Ã­t nháº¥t 3 cÃ¢u Ä‘á»ƒ thá»±c hiá»‡n phÃ¢n cá»¥m.")
            return

        vectors = encode_sentences(sentences)
        max_k = min(8, len(sentences))
        K, distortions, silhouette_scores = find_optimal_clusters(vectors, max_k)

        if not K:
            st.error("KhÃ´ng tÃ¬m Ä‘Æ°á»£c cá»¥m há»£p lá»‡.")
            return

        # Biá»ƒu Ä‘á»“ Elbow + Silhouette
        fig, ax = plt.subplots(1, 2, figsize=(12, 4))
        ax[0].plot(K, distortions, marker='o')
        ax[0].set_title("Elbow Method")
        ax[0].set_xlabel("Sá»‘ cá»¥m")
        ax[0].set_ylabel("Distortion")

        ax[1].plot(K, silhouette_scores, marker='x', color='green')
        ax[1].set_title("Silhouette Score")
        ax[1].set_xlabel("Sá»‘ cá»¥m")
        ax[1].set_ylabel("Score")

        st.pyplot(fig)

        optimal_k = K[silhouette_scores.index(max(silhouette_scores))]
        st.info(f"âœ… Sá»‘ cá»¥m tá»‘i Æ°u: {optimal_k}")

        kmeans = cluster_sentences(vectors, num_clusters=optimal_k)
        summary = summarize_by_clustering(sentences, kmeans)
        st.subheader("ðŸ“š Báº£n tÃ³m táº¯t theo phÃ¢n cá»¥m:")
        st.write(summary)

        # PCA/TSNE
        if st.checkbox("ðŸ“Š Hiá»ƒn thá»‹ cá»¥m ná»™i dung"):
            method = st.selectbox("PhÆ°Æ¡ng phÃ¡p giáº£m chiá»u:", ["pca", "tsne"])
            plot_clusters(vectors, kmeans.labels_, method=method)

        # TÃ³m táº¯t siÃªu ngáº¯n
        if st.checkbox("âœ¨ Táº¡o báº£n tÃ³m táº¯t siÃªu ngáº¯n (há»c sÃ¢u)"):

            # Heuristic Search (TF-IDF + Cosine Similarity)
            tfidf_matrix = compute_tfidf(sentences)
            cosine_sim = compute_cosine_similarity(tfidf_matrix)
            heuristic_summary = heuristic_search(sentences, cosine_sim)
            st.subheader("ðŸ“š TÃ³m táº¯t báº±ng Heuristic Search:")
            st.write(heuristic_summary)

            with st.spinner("Äang cháº¡y mÃ´ hÃ¬nh há»c sÃ¢u..."):
                model = load_finetuned_model()
                short_summary = summarize_with_bart(text_input, model)
                st.success("ðŸ“ TÃ³m táº¯t siÃªu ngáº¯n:")
                st.write(short_summary)

if __name__ == "__main__":
    main()
