{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Id', 'URL', 'Title', 'Summary', 'Contents', 'Date', 'Author(s)', 'Category', 'Tags'],\n",
      "    num_rows: 313320\n",
      "})\n",
      "D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh Dataset_articles_converted.csv\n",
      "D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh Dataset_articles_converted.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313320/313320 [00:06<00:00, 49586.33 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c Dataset_articles_hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # type: ignore\n",
    "from datasets import Dataset\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu\n",
    "df = pd.read_csv(\"Dataset_articles.csv\")\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang ƒë·ªãnh d·∫°ng Dataset c·ªßa Hugging Face\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Ki·ªÉm tra d·ªØ li·ªáu\n",
    "print(dataset)\n",
    "#csv\n",
    "df.to_csv(\"Dataset_articles_converted.csv\", index=False)\n",
    "print(\"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh Dataset_articles_converted.csv\")\n",
    "#json\n",
    "df.to_json(\"Dataset_articles_converted.json\", orient=\"records\", lines=True)\n",
    "print(\"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh Dataset_articles_converted.json\")\n",
    "#arrow\n",
    "dataset.save_to_disk(\"Dataset_articles_hf\")\n",
    "print(\"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c Dataset_articles_hf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id                                                URL  \\\n",
      "0   0  https://laodong.vn/bat-dong-san/thong-tin-ngoc...   \n",
      "1   1  https://laodong.vn/bat-dong-san/lo-hong-trong-...   \n",
      "2   2  https://laodong.vn/bat-dong-san/som-hoan-thien...   \n",
      "3   3  https://laodong.vn/bat-dong-san/chi-tiet-ho-so...   \n",
      "4   4  https://laodong.vn/bat-dong-san/khoi-tao-khong...   \n",
      "\n",
      "                                               Title  \\\n",
      "0  Th√¥ng tin ‚ÄúNg·ªçc Trinh mua ƒë·∫•t ·ªü B·∫£o L·ªôc\" ch·ªâ l...   \n",
      "1  L·ªó h·ªïng trong vi·ªác th·∫©m tra nƒÉng l·ª±c t√†i ch√≠nh...   \n",
      "2  S·ªõm ho√†n thi·ªán c√°c d·ª± √°n nh√† ·ªü x√£ h·ªôi ƒë·ªÉ CNLƒê ...   \n",
      "3            Chi ti·∫øt h·ªì s∆° ho√†n c√¥ng nh√† ·ªü nƒÉm 2022   \n",
      "4  Kh·ªüi t·∫°o kh√¥ng gian s·ªëng ƒë·∫≥ng c·∫•p, ƒë√≥n s√≥ng ƒë·∫ß...   \n",
      "\n",
      "                                             Summary  \\\n",
      "0  L√¢m¬†ƒê·ªìng -¬†L√£nh ƒë·∫°o th√†nh ph·ªë B·∫£o L·ªôc, L√¢m ƒê·ªìn...   \n",
      "1  TPHCM - Vi·ªác kh√¥ng th·ªÉ c∆∞·ª°ng ch·∫ø thu·∫ø c·ªßa hai ...   \n",
      "2  Hi·ªán tr√™n ƒë·ªãa b√†n t·ªânh Ninh B√¨nh c√≥ 32 khu, c·ª•...   \n",
      "3  Ho√†n c√¥ng nh√† ·ªü v·ªõi √Ω nghƒ©a l√† ƒëi·ªÅu ki·ªán ƒë·ªÉ ƒë∆∞...   \n",
      "4  C√≥ r·∫•t nhi·ªÅu l√Ω do khi·∫øn nh·ªØng d·ª± √°n th·∫•p n·ªôi ...   \n",
      "\n",
      "                                            Contents  \\\n",
      "0  Nh·ªØng ng√†y¬†v·ª´a¬†qua, tr√™n¬†trang Facebook ch√≠nh ...   \n",
      "1  Theo th√¥ng tin t·ª´ C·ª•c Thu·∫ø TP.HCM, hi·ªán c∆° qua...   \n",
      "2  CNLƒê mong mu·ªën s·ªõm ƒë∆∞·ª£c ti·∫øp c·∫≠n v·ªõi nh√† ·ªü x√£ ...   \n",
      "3  Ho√†n c√¥ng nh√† ·ªü l√† m·ªôt th·ªß t·ª•c h√†nh ch√≠nh tron...   \n",
      "4  ƒêi d·ªçc ƒë∆∞·ªùng L√™ VƒÉn L∆∞∆°ng k√©o d√†i xu·ªëng khu D∆∞...   \n",
      "\n",
      "                                Date        Author(s)      Category  \\\n",
      "0  Th·ª© s√°u, 20/05/2022 08:56 (GMT+7)     Ph∆∞∆°ng Nhi√™n  B·∫•t ƒë·ªông s·∫£n   \n",
      "1  Th·ª© s√°u, 20/05/2022 08:10 (GMT+7)         Gia Mi√™u  B·∫•t ƒë·ªông s·∫£n   \n",
      "2  Th·ª© s√°u, 20/05/2022 07:47 (GMT+7)    NGUY·ªÑN TR∆Ø·ªúNG  B·∫•t ƒë·ªông s·∫£n   \n",
      "3  Th·ª© s√°u, 20/05/2022 06:44 (GMT+7)  Kim Nhung (T/H)  B·∫•t ƒë·ªông s·∫£n   \n",
      "4  Th·ª© nƒÉm, 19/05/2022 15:30 (GMT+7)     Huy·ªÅn Nguy·ªÖn  B·∫•t ƒë·ªông s·∫£n   \n",
      "\n",
      "                                                Tags  \n",
      "0  ['L√¢m ƒê·ªìng', 'Ng·ªçc Trinh', 'Chi√™u tr√≤', 'Gi√° ƒë...  \n",
      "1                       ['Th·ªß Thi√™m', 'ƒê·∫•u gi√° ƒë·∫•t']  \n",
      "2  ['D·ª± √°n', 'Nh√† ·ªü x√£ h·ªôi', 'D·ª± √°n nh√† ·ªü x√£ h·ªôi'...  \n",
      "3  ['Gi·∫•y ph√©p x√¢y d·ª±ng', 'H·ªì s∆° ho√†n c√¥ng', 'nh√†...  \n",
      "4                                   ['An Qu√Ω Villa']  \n"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc l·∫°i file ƒë·ªÉ ki·ªÉm tra\n",
    "df_check = pd.read_csv(\"./data/Dataset_articles_converted.csv\")\n",
    "print(df_check.head())  # Hi·ªÉn th·ªã 5 d√≤ng ƒë·∫ßu ti√™n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh Dataset_articles_converted.csv\n",
      "D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh Dataset_articles_converted.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313320/313320 [00:01<00:00, 215684.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c Dataset_articles_hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# ƒê·ªçc file CSV (thay 'your_file.csv' b·∫±ng t√™n file c·ªßa b·∫°n)\n",
    "df = pd.read_csv(\"./data/Dataset_articles.csv\")\n",
    "\n",
    "# Gi·ªØ l·∫°i ch·ªâ hai c·ªôt 'contents' v√† 'summary'\n",
    "df = df[['Contents', 'Summary']]\n",
    "\n",
    "# L∆∞u l·∫°i file CSV m·ªõi (n·∫øu c·∫ßn)\n",
    "df.to_csv(\"Dataset_articles_converted.csv\", index=False)\n",
    "print(\"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh Dataset_articles_converted.csv\")\n",
    "\n",
    "# L∆∞u d·ªØ li·ªáu d∆∞·ªõi d·∫°ng JSON (d√≤ng-by-d√≤ng)\n",
    "df.to_json(\"Dataset_articles_converted.json\", orient=\"records\", lines=True)\n",
    "print(\"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh Dataset_articles_converted.json\")\n",
    "\n",
    "# Chuy·ªÉn DataFrame pandas th√†nh Dataset c·ªßa Hugging Face\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# L∆∞u l·∫°i d∆∞·ªõi ƒë·ªãnh d·∫°ng Arrow\n",
    "dataset.save_to_disk(\"Dataset_articles_hf\")\n",
    "print(\"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o th∆∞ m·ª•c Dataset_articles_hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Contents': 'Nh·ªØng ng√†y\\xa0v·ª´a\\xa0qua, tr√™n\\xa0trang Facebook ch√≠nh ch·ªß c·ªßa Ng·ªçc Trinh c√≥ ƒëƒÉng t·∫£i th√¥ng tin ng∆∞·ªùi m·∫´u n√†y v·ª´a mua\\xa011ha ƒë·∫•t v·ªõi homestay\\xa0t·∫°i x√£ ƒê·∫°i L√†o, TP.B·∫£o L·ªôc, thu h√∫t s·ª± ch√∫ √Ω l·ªõn t·ª´ ƒë√¥ng ƒë·∫£o c·ªông ƒë·ªìng m·∫°ng.\\xa0 Sau\\xa0ƒë√≥, m·ªôt s·ªë trang th√¥ng tin v√† m·∫°ng\\xa0x√£ h·ªôi kh√°c ti·∫øp t·ª•c ƒëƒÉng t·∫°i l·∫°i th√¥ng tin n√†y,\\xa0ƒë·ªìng th·ªùi kh·∫≥ng ƒë·ªãnh, khu ƒë·∫•t Ng·ªçc Trinh m·ªõi mua n·∫±m t·∫°i x√£ ƒê·∫°i L√†o, TP.B·∫£o L·ªôc,\\xa0t·ªça l·∫°c ·ªü v·ªã tr√≠ ƒë·∫Øc ƒë·ªãa, ti·∫øp gi√°p n√∫i v√† c√≥ kh√≠ h·∫≠u m√°t m·∫ª d·ªÖ ch·ªãu, th√≠ch h·ª£p cho k·ª≥ ngh·ªâ tr·ªën n·∫Øng h√® cho du kh√°ch. Tr∆∞·ªõc\\xa0th√¥ng tin n√†y, √¥ng ƒêo√†n Kim ƒê√¨nh - Ch·ªß t·ªãch UBND th√†nh ph·ªë B·∫£o L·ªôc cho\\xa0bi·∫øt,\\xa0UBND th√†nh ph·ªë ƒë√£ y√™u c·∫ßu c∆° quan ch·ª©c nƒÉng v√† UBND x√£ ƒê·∫°i L√†o ph·ªëi h·ª£p ki·ªÉm ch·ª©ng th√¥ng tin. Qua x√°c minh,\\xa0khu ƒë·∫•t Ng·ªçc Trinh ch·ª•p h√¨nh v√†\\xa0ƒëƒÉng t·∫£i l√™n m·∫°ng x√£ h·ªôi\\xa0thu·ªôc h·∫ªm 61 B‚ÄôLao Sr√™,\\xa0x√£ ƒê·∫°i L√†o. Nh∆∞ng ƒë√¢y kh√¥ng ph·∫£i l√† ƒë·∫•t x√¢y d·ª±ng homestay c·ªßa Ng·ªçc Trinh. Trong c√°c h·ªì s∆° th·ªß t·ª•c li√™n quan ƒë·∫•t ƒëai tr√™n ƒë·ªãa b√†n x√£ ƒê·∫°i L√†o kh√¥ng c√≥ ch·ªß nh√¢n n√†o t√™n l√† Tr·∫ßn Th·ªã Ng·ªçc Trinh (t√™n th·∫≠t\\xa0c·ªßa\\xa0Ng·ªçc Trinh).\\xa0 M·∫∑t kh√°c t·∫•t c·∫£ c√°c d·ª± √°n x√¢y d·ª±ng homestay ngh·ªâ d∆∞·ª°ng tr√™n ƒë·ªãa b√†n th√†nh ph·ªë B·∫£o L·ªôc ƒë·ªÅu ƒë∆∞·ª£c ƒë·ªãa ph∆∞∆°ng c·∫•p ph√©p v√† qu·∫£n l√Ω, trong c√°c d·ª± √°n ƒë√£ tri·ªÉn khai v√† ƒëang ch·ªù ph√™ duy·ªát kh√¥ng c√≥ d·ª± √°n n√†o c√≥ ch·ªß d·ª± √°n t√™n Ng·ªçc Trinh. ƒê·∫°i\\xa0di·ªán l√£nh ƒë·∫°o UBND TP.B·∫£o L·ªôc c≈©ng n√≥i th√™m,\\xa0ƒë√¢y c√≥ th·ªÉ l√† chi√™u tr√≤ c√¢u like c·ªßa gi·ªõi b·∫•t ƒë·ªông s·∫£n. Vi·ªác n√†y, khi·∫øn d∆∞ lu·∫≠n ƒë√°nh gi√° sai v·ªÅ c√¥ng t√°c qu·∫£n l√Ω ƒë·∫•t ƒëai, x√¢y d·ª±ng c·ªßa ƒë·ªãa ph∆∞∆°ng v√† c∆° quan ch·ª©c nƒÉng tr√™n ƒë·ªãa b√†n th√†nh ph·ªë B·∫£o L·ªôc; ƒë·ªìng th·ªùi, c≈©ng l√† chi√™u tr√≤ m√† gi·ªõi b·∫•t ƒë·ªông s·∫£n h∆∞·ªõng t·ªõi ƒë·ªÉ th·ªïi ph·ªìng gi√° ƒë·∫•t.\\xa0 Hi·ªán t·∫°i, UBND th√†nh ph·ªë B·∫£o L·ªôc ƒëang ch·ªâ ƒë·∫°o c√°c ng√†nh ch·ª©c nƒÉng v√† UBND x√£ ƒê·∫°i L√†o x√°c minh, n·∫øu ƒë√¢y l√† chi√™u tr√≤ c√¢u like sai s·ª± th·∫≠t th√¨ ƒë·ªãa ph∆∞∆°ng ki√™n quy·∫øt x·ª≠ l√Ω theo quy ƒë·ªãnh. Ng∆∞·ªùi d√¢n c≈©ng c·∫ßn h·∫øt s·ª©c c·∫©n th·∫≠n, ki·ªÉm ch·ª©ng k·ªπ c√†ng ƒë·ªÉ tr√°nh b·ªã d·∫Øt m≈©i b·ªüi nh·ªØng th√¥ng tin ƒë∆∞·ª£c c√°c c√° nh√¢n ƒëƒÉng t·∫£i t·ª± do nh∆∞ th·∫ø n√†y.', 'Summary': 'L√¢m\\xa0ƒê·ªìng -\\xa0L√£nh ƒë·∫°o th√†nh ph·ªë B·∫£o L·ªôc, L√¢m ƒê·ªìng\\xa0kh·∫≥ng ƒë·ªãnh th√¥ng\\xa0tin\\xa0ng∆∞·ªùi m·∫´u Ng·ªçc Trinh mua 11ha ƒë·∫•t ·ªü ƒë·ªãa ph∆∞∆°ng ƒë·ªÉ l√†m homestay l√†\\xa0ho√†n to√†n sai s·ª± th·∫≠t.\\xa0ƒê√≥\\xa0ch·ªâ l√† chi√™u tr√≤ c·ªßa\\xa0gi·ªõi b·∫•t ƒë·ªông s·∫£n\\xa0ƒë·ªÉ th·ªïi gi√° ƒë·∫•t.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load dataset t·ª´ th∆∞ m·ª•c ƒë√£ l∆∞u\n",
    "dataset = load_from_disk(\"../data/Dataset_articles_hf\")\n",
    "\n",
    "# Ki·ªÉm tra m·ªôt s·ªë m·∫´u\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: d:\\study\\mon_ky_5\\tri_tue_nhan_tao\\llmprojects\\notebooks\n",
      "C√°c file trong th∆∞ m·ª•c hi·ªán t·∫°i: ['fine_tune_t5.ipynb', 'text_summarize.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"C√°c file trong th∆∞ m·ª•c hi·ªán t·∫°i:\", os.listdir(\".\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: './data'"
     ]
    }
   ],
   "source": [
    "os.listdir(\"./data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset_articles.csv',\n",
       " 'Dataset_articles_converted.csv',\n",
       " 'Dataset_articles_converted.json',\n",
       " 'Dataset_articles_hf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒëo·∫°n tr√™n l√† t ki·ªÉm tra data nh√© data oke r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ch·ªØ in ƒë·∫≠m**  \n",
    "*Ch·ªØ in nghi√™ng*  \n",
    "`M√£ code`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize d·ªØ li·ªáu v·ªõi tokenizer c·ªßa T5\n",
    "M√¥ h√¨nh T5 y√™u c·∫ßu tokenization cho c·∫£ vƒÉn b·∫£n g·ªëc v√† vƒÉn b·∫£n t√≥m t·∫Øt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"t5-small\"  # Ho·∫∑c t5-base, t5-large n·∫øu c√≥ ƒë·ªß t√†i nguy√™n\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# H√†m ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"Contents\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    # Tokenize ph·∫ßn t√≥m t·∫Øt (labels)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"Summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω l√™n dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291c790ba6024d1b8a7372d44ab85c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\haidu\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd9ef24502344e788985a9d2c26bbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c3610bbc6440a989dc526a3f81b234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23fe9fddcd3489dbd080b5da5b41bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/313320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω l√™n dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m tokenized_datasets = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:602\u001b[39m, in \u001b[36mtransmit_tasks.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    600\u001b[39m     \u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    601\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m datasets: List[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    604\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[32m    605\u001b[39m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    560\u001b[39m self_format = {\n\u001b[32m    561\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    562\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    563\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    564\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    565\u001b[39m }\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m datasets: List[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:3156\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n\u001b[32m   3150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3151\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3152\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3153\u001b[39m         total=pbar_total,\n\u001b[32m   3154\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3155\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3157\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3158\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:3547\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[39m\n\u001b[32m   3543\u001b[39m indices = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   3544\u001b[39m     \u001b[38;5;28mrange\u001b[39m(*(\u001b[38;5;28mslice\u001b[39m(i, i + batch_size).indices(shard.num_rows)))\n\u001b[32m   3545\u001b[39m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[32m   3546\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3547\u001b[39m     batch = \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3551\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3553\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[32m   3554\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[32m   3555\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3556\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:3416\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[39m\u001b[34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[39m\n\u001b[32m   3414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[32m   3415\u001b[39m     additional_args += (rank,)\n\u001b[32m-> \u001b[39m\u001b[32m3416\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[32m   3418\u001b[39m     processed_inputs = {\n\u001b[32m   3419\u001b[39m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs.data.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs.keys_to_format\n\u001b[32m   3420\u001b[39m     }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mpreprocess_function\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mpreprocess_function\u001b[39m(examples):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     inputs = [\u001b[33;43m\"\u001b[39;49m\u001b[33;43msummarize: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[33m\"\u001b[39m\u001b[33mContents\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     10\u001b[39m     model_inputs = tokenizer(inputs, max_length=\u001b[32m512\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Tokenize ph·∫ßn t√≥m t·∫Øt (labels)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"t5-small\"  # Ho·∫∑c t5-base, t5-large n·∫øu c√≥ ƒë·ªß t√†i nguy√™n\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# H√†m ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"Contents\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    # Tokenize ph·∫ßn t√≥m t·∫Øt (labels)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"Summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω l√™n dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gi√° tr·ªã kh√¥ng ph·∫£i chu·ªói ho·∫∑c gi√° tr·ªã b·ªã thi·∫øu (missing values)\n",
    "V·∫•n ƒë·ªÅ: N·∫øu m·ªôt s·ªë m·ª•c trong c·ªôt ch·ª©a gi√° tr·ªã None ho·∫∑c ki·ªÉu d·ªØ li·ªáu kh√¥ng ph·∫£i l√† chu·ªói, ph√©p n·ªëi \"summarize: \" + text s·∫Ω g√¢y ra l·ªói.\n",
    "\n",
    "Gi·∫£i ph√°p: √âp t·∫•t c·∫£ c√°c gi√° tr·ªã th√†nh chu·ªói v√† x·ª≠ l√Ω tr∆∞·ªùng h·ª£p gi√° tr·ªã b·ªã thi·∫øu. V√≠ d·ª•:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # S·ª≠ d·ª•ng \"Contents\" v√† \"Summary\" theo ƒë√∫ng t√™n trong dataset\n",
    "    inputs = [\"summarize: \" + str(text) if text is not None else \"summarize:\" \n",
    "              for text in examples[\"Contents\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    \n",
    "    # √âp ki·ªÉu gi√° tr·ªã c·ªßa \"Summary\" v·ªÅ chu·ªói v√† x·ª≠ l√Ω gi√° tr·ªã None\n",
    "    summaries = [str(summary) if summary is not None else \"\" for summary in examples[\"Summary\"]]\n",
    "    labels = tokenizer(summaries, max_length=150, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e38358afa449c6af1cdaab0bdcfc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/313320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Import v√† Load Tokenizer\n",
    "python\n",
    "Sao ch√©p\n",
    "Ch·ªânh s·ª≠a\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"t5-small\"  # Ho·∫∑c t5-base, t5-large n·∫øu c√≥ ƒë·ªß t√†i nguy√™n\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "T5Tokenizer: D√πng ƒë·ªÉ m√£ h√≥a (tokenize) vƒÉn b·∫£n th√†nh s·ªë.\n",
    "\n",
    "from_pretrained(model_name): T·∫£i tokenizer c·ªßa m√¥ h√¨nh t5-small. N·∫øu t√†i nguy√™n m·∫°nh h∆°n, c√≥ th·ªÉ d√πng t5-base ho·∫∑c t5-large.\n",
    "\n",
    "2Ô∏è‚É£ H√†m ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "python\n",
    "Sao ch√©p\n",
    "Ch·ªânh s·ª≠a\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"Contents\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "M·ª•c ƒë√≠ch: Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o cho m√¥ h√¨nh.\n",
    "\n",
    "examples[\"Contents\"]: Ch·ª©a danh s√°ch c√°c vƒÉn b·∫£n g·ªëc.\n",
    "\n",
    "Ti·ªÅn t·ªë \"summarize: \": T5 s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p prompt-based, n√™n c·∫ßn th√™m \"summarize: \" v√†o tr∆∞·ªõc m·ªói vƒÉn b·∫£n ƒë·ªÉ m√¥ h√¨nh hi·ªÉu r·∫±ng ƒë√¢y l√† t√°c v·ª• t√≥m t·∫Øt.\n",
    "\n",
    "Tokenize vƒÉn b·∫£n v·ªõi max_length=512, ƒë·ªìng th·ªùi c·∫Øt b·ªõt (truncation) n·∫øu d√†i h∆°n.\n",
    "\n",
    "3Ô∏è‚É£ Tokenize ph·∫ßn t√≥m t·∫Øt\n",
    "python\n",
    "Sao ch√©p\n",
    "Ch·ªânh s·ª≠a\n",
    "    # Tokenize ph·∫ßn t√≥m t·∫Øt (labels)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"Summary\"], max_length=128, truncation=True)\n",
    "examples[\"Summary\"]: Ch·ª©a danh s√°ch c√°c b·∫£n t√≥m t·∫Øt t∆∞∆°ng ·ª©ng.\n",
    "\n",
    "tokenizer.as_target_tokenizer(): C·∫•u h√¨nh tokenizer ƒë·ªÉ m√£ h√≥a ph·∫ßn ƒë·∫ßu ra (target/labels).\n",
    "\n",
    "Tokenize ph·∫ßn t√≥m t·∫Øt v·ªõi max_length=128 (ng·∫Øn h∆°n input v√¨ t√≥m t·∫Øt th∆∞·ªùng ng·∫Øn h∆°n vƒÉn b·∫£n g·ªëc).\n",
    "\n",
    "4Ô∏è‚É£ G√°n labels v√†o d·ªØ li·ªáu ƒë·∫ßu v√†o\n",
    "python\n",
    "Sao ch√©p\n",
    "Ch·ªânh s·ª≠a\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "G√°n labels[\"input_ids\"] v√†o model_inputs ƒë·ªÉ m√¥ h√¨nh c√≥ th·ªÉ h·ªçc c√°ch √°nh x·∫° t·ª´ vƒÉn b·∫£n g·ªëc sang b·∫£n t√≥m t·∫Øt.\n",
    "\n",
    "5Ô∏è‚É£ √Åp d·ª•ng h√†m ti·ªÅn x·ª≠ l√Ω l√™n to√†n b·ªô dataset\n",
    "python\n",
    "Sao ch√©p\n",
    "Ch·ªânh s·ª≠a\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "dataset.map(preprocess_function, batched=True): √Åp d·ª•ng preprocess_function l√™n to√†n b·ªô dataset theo t·ª´ng batch (tƒÉng t·ªëc x·ª≠ l√Ω).\n",
    "\n",
    "K·∫øt qu·∫£: tokenized_datasets s·∫Ω ch·ª©a d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c m√£ h√≥a, s·∫µn s√†ng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh.\n",
    "\n",
    "üî• T√≥m t·∫Øt ƒë∆°n gi·∫£n\n",
    "Th√™m ti·ªÅn t·ªë \"summarize: \" v√†o m·ªói vƒÉn b·∫£n.\n",
    "\n",
    "Tokenize vƒÉn b·∫£n g·ªëc (input) v√† b·∫£n t√≥m t·∫Øt (labels).\n",
    "\n",
    "G√°n labels v√†o ƒë·∫ßu v√†o ƒë·ªÉ m√¥ h√¨nh h·ªçc c√°ch t√≥m t·∫Øt.\n",
    "\n",
    "√Åp d·ª•ng l√™n dataset ƒë·ªÉ chu·∫©n b·ªã cho hu·∫•n luy·ªán.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1bed0ed427424fa507fbff65ef5d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/251 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a6f686c94a429a9494ecf95f4dc78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/63 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534445ae6a63440f836de2554dffd582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/251 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544a2d5aa91149638786c8a8d7a41fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/63 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "\n",
    "\n",
    "train_dataset.to_csv(\"train_data.csv\")\n",
    "eval_dataset.to_csv(\"eval_data.csv\")\n",
    "\n",
    "\n",
    "train_dataset.to_json(\"train_data.json\")\n",
    "eval_dataset.to_json(\"eval_data.json\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"train_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "\n",
    "with open(\"eval_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(eval_dataset, f)\n",
    "\n",
    "#D√πng .to_csv() ho·∫∑c .to_json() n·∫øu mu·ªën l∆∞u ·ªü d·∫°ng file vƒÉn b·∫£n.\n",
    "#D√πng pickle n·∫øu mu·ªën load nhanh h∆°n trong Python. l∆∞u d·∫°ng n√†y th√¨ k xem ƒëc nh√© l∆∞u nh·ªã ph√¢n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "\n",
    "# Load m√¥ h√¨nh T5\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# C·∫•u h√¨nh Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,   # Gi·∫£m batch size ƒë·ªÉ tr√°nh tr√†n VRAM\n",
    "    per_device_eval_batch_size=2,    # Gi·∫£m batch size khi eval\n",
    "    gradient_accumulation_steps=4,   # TƒÉng ƒë·ªÉ b√π l·∫°i batch size nh·ªè\n",
    "    learning_rate=3e-5,              # Gi·∫£m learning rate ƒë·ªÉ train ·ªïn ƒë·ªãnh h∆°n\n",
    "    num_train_epochs=5,              # TƒÉng epochs n·∫øu batch size nh·ªè\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,               # Gi·∫£m logging ƒë·ªÉ tƒÉng t·ªëc training\n",
    "    save_total_limit=1,              # Ch·ªâ l∆∞u 1 checkpoint ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ\n",
    "    fp16=True,                       # D√πng float16 ƒë·ªÉ gi·∫£m t·∫£i cho GPU (n·∫øu h·ªó tr·ª£)\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "\n",
    "# T·∫°o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    # L∆∞u checkpoint theo steps thay v√¨ epoch\n",
    "    save_strategy=\"steps\",      \n",
    "    save_steps=1000,            # M·ªói 1000 steps l∆∞u 1 checkpoint\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,          # C·ª© 200 steps th√¨ log 1 l·∫ßn\n",
    "    save_total_limit=1,         # Ch·ªâ gi·ªØ l·∫°i 1 checkpoint m·ªõi nh·∫•t (ƒë·ªÉ ti·∫øt ki·ªám dung l∆∞·ª£ng)\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    # N·∫øu mu·ªën eval m·ªói 1000 steps, d√πng eval_strategy=\"steps\" + eval_steps=1000\n",
    "    eval_strategy=\"steps\",      \n",
    "    eval_steps=1000\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74f6cd7bb8a4db3b7294af2770129ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/156660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ Training ho√†n t·∫•t!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2279\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2276\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2282\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2285\u001b[39m ):\n\u001b[32m   2286\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2287\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3318\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs)\u001b[39m\n\u001b[32m   3315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3317\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3318\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3320\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3322\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3323\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3324\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3363\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs)\u001b[39m\n\u001b[32m   3361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3362\u001b[39m     labels = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3363\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3364\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3365\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1702\u001b[39m, in \u001b[36mT5ForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1699\u001b[39m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[32m   1700\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1701\u001b[39m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1702\u001b[39m     encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1703\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[32m   1712\u001b[39m     encoder_outputs = BaseModelOutput(\n\u001b[32m   1713\u001b[39m         last_hidden_state=encoder_outputs[\u001b[32m0\u001b[39m],\n\u001b[32m   1714\u001b[39m         hidden_states=encoder_outputs[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1715\u001b[39m         attentions=encoder_outputs[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1716\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1106\u001b[39m, in \u001b[36mT5Stack.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1091\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1092\u001b[39m         layer_module.forward,\n\u001b[32m   1093\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1103\u001b[39m         output_attentions,\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1106\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[32m   1121\u001b[39m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:746\u001b[39m, in \u001b[36mT5Block.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[39m\n\u001b[32m    743\u001b[39m     attention_outputs = attention_outputs + cross_attention_outputs[\u001b[32m2\u001b[39m:]\n\u001b[32m    745\u001b[39m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hidden_states.dtype == torch.float16:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:335\u001b[39m, in \u001b[36mT5LayerFF.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[32m    334\u001b[39m     forwarded_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     forwarded_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.dropout(forwarded_states)\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:282\u001b[39m, in \u001b[36mT5DenseActDense.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    280\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.wi(hidden_states)\n\u001b[32m    281\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.act(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    284\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.wo.weight, torch.Tensor)\n\u001b[32m    285\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m hidden_states.dtype != \u001b[38;5;28mself\u001b[39m.wo.weight.dtype\n\u001b[32m    286\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.wo.weight.dtype != torch.int8\n\u001b[32m    287\u001b[39m ):\n\u001b[32m    288\u001b[39m     hidden_states = hidden_states.to(\u001b[38;5;28mself\u001b[39m.wo.weight.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\haidu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1426\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "print(\"üöÄ Training ho√†n t·∫•t!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
