import streamlit as st  # type: ignore
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics.pairwise import cosine_similarity
from transformers import T5Tokenizer, T5ForConditionalGeneration


# Táº£i punkt
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Danh sÃ¡ch cÃ¡c tá»« ná»‘i cáº§n loáº¡i bá»
stopwords = [
    "trÆ°á»›c khi", "sau khi", "trong khi", "khi mÃ ", "lÃºc Ä‘Ã³", "bÃ¢y giá»", "hiá»‡n táº¡i", "lÃºc nÃ y", "ngay láº­p tá»©c",
    "bá»Ÿi vÃ¬", "vÃ¬", "do", "bá»Ÿi", "káº¿t quáº£ lÃ ", "dáº«n Ä‘áº¿n", "vÃ¬ láº½ Ä‘Ã³", "chÃ­nh vÃ¬ tháº¿", "tá»« Ä‘Ã³",
    "nhÆ°ng", "nhÆ°ng mÃ ", "tuy", "máº·c dÃ¹ váº­y", "trÃ¡i láº¡i", "ngÆ°á»£c láº¡i", "tuy váº­y", "dÃ¹ váº­y", "tuy tháº¿",
    "ngoÃ i ra", "thÃªm vÃ o Ä‘Ã³", "hÆ¡n ná»¯a", "khÃ´ng nhá»¯ng tháº¿", "bÃªn cáº¡nh Ä‘Ã³", "cÅ©ng nhÆ°", "Ä‘á»“ng thá»i",
    "náº¿u", "náº¿u nhÆ°", "giáº£ sá»­", "vá»›i Ä‘iá»u kiá»‡n", "trong trÆ°á»ng há»£p", "miá»…n lÃ ", "cho dÃ¹", "dÃ¹ cho",
    "Ä‘á»ƒ", "nháº±m", "vá»›i má»¥c Ä‘Ã­ch", "nháº±m má»¥c Ä‘Ã­ch", "háº§u", "Ä‘á»ƒ mÃ ", "nháº±m má»¥c tiÃªu",
    "giá»‘ng nhÆ°", "tÆ°Æ¡ng tá»±", "cÅ©ng giá»‘ng", "khÃ¡c vá»›i", "so vá»›i", "hÆ¡n", "kÃ©m", "nhÆ° lÃ ",
    "hay lÃ ", "sau Ä‘Ã³", "vÃ¬ váº­y", "cho nÃªn", "tuy nhiÃªn", "máº·c dÃ¹", "do Ä‘Ã³", "vÃ¬ tháº¿", "tháº¿ nhÆ°ng",
    "bÃªn cáº¡nh Ä‘Ã³", "ngoÃ i ra", "Ä‘á»“ng thá»i", "trong khi Ä‘Ã³", "hÆ¡n ná»¯a", "cÃ³ thá»ƒ nÃ³i",
    "máº·t khÃ¡c", "tá»« Ä‘Ã³", "trÃªn thá»±c táº¿", "káº¿t luáº­n láº¡i", "cá»¥ thá»ƒ lÃ ", "nÃ³i chung",
    "nhÃ¬n chung", "nÃ³i cÃ¡ch khÃ¡c", "tÃ³m láº¡i", "tá»« trÆ°á»›c Ä‘áº¿n nay", "vá» cÆ¡ báº£n", "thá»±c ra",
    "Ä‘iá»u Ä‘Ã³ cho tháº¥y", "nhÆ° váº­y", "káº¿t quáº£ lÃ ", "suy ra", "nÃ³i tÃ³m láº¡i"
]

# --- Tiá»n xá»­ lÃ½ ---
def tokenize_sentences(text):
    return nltk.sent_tokenize(text)

def remove_stopwords(sentences):
    cleaned_sentences = []
    for sentence in sentences:
        cleaned_sentence = sentence
        for stopword in stopwords:
            cleaned_sentence = cleaned_sentence.replace(stopword, "")
        cleaned_sentences.append(cleaned_sentence)
    return cleaned_sentences

def encode_sentences(sentences):
    vectorizer = TfidfVectorizer()
    return vectorizer.fit_transform(sentences)

# --- PhÃ¢n cá»¥m ---
def find_optimal_clusters(sentence_vectors, max_k=8):
    distortions, silhouette_scores, K_valid = [], [], []
    for k in range(2, max_k + 1):
        if k >= sentence_vectors.shape[0]:
            break
        kmeans = KMeans(n_clusters=k, random_state=0).fit(sentence_vectors)
        labels = kmeans.labels_
        if len(set(labels)) < 2: continue
        distortions.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(sentence_vectors, labels))
        K_valid.append(k)
    return K_valid, distortions, silhouette_scores

def cluster_sentences(sentence_vectors, num_clusters):
    kmeans = KMeans(n_clusters=num_clusters, random_state=0)
    kmeans.fit(sentence_vectors)
    return kmeans

def summarize_by_clustering(sentences, kmeans):
    labels = kmeans.labels_
    summary = []
    for cluster in set(labels):
        indices = [i for i, label in enumerate(labels) if label == cluster]
        summary.append(sentences[indices[0]])  # CÃ¢u Ä‘áº¡i diá»‡n má»—i cá»¥m
    return ' '.join(summary)

# --- Heuristic Search ---
def compute_tfidf(sentences):
    vectorizer = TfidfVectorizer()
    return vectorizer.fit_transform(sentences)

def compute_cosine_similarity(tfidf_matrix):
    return cosine_similarity(tfidf_matrix, tfidf_matrix)

def heuristic_search(sentences, cosine_sim, num_summary_sentences=3):
    sentence_scores = cosine_sim.sum(axis=1)
    ranked_sentences = sentence_scores.argsort()[::-1]
    summary = [sentences[i] for i in ranked_sentences[:num_summary_sentences]]
    return ' '.join(summary)

# --- MÃ´ hÃ¬nh há»c sÃ¢u ---
@st.cache_resource
def load_finetuned_model(model_path="duonggbill/dbill-model-summary"):  # <-- dÃ¹ng Hugging Face model
    tokenizer = T5Tokenizer.from_pretrained(model_path)
    model = T5ForConditionalGeneration.from_pretrained(model_path)
    return model, tokenizer

def summarize_with_t5(text, model, tokenizer):
    input_ids = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(input_ids, max_length=150, min_length=80, length_penalty=2.0, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# --- TÃ³m táº¯t káº¿t há»£p ---
def hybrid_summarization(text, model, tokenizer, num_summary_sentences=None):
    sentences = tokenize_sentences(text)
    cleaned_sentences = remove_stopwords(sentences)

    if len(cleaned_sentences) < 2:
        return "VÄƒn báº£n quÃ¡ ngáº¯n Ä‘á»ƒ tÃ³m táº¯t."

    # --- XÃ¡c Ä‘á»‹nh sá»‘ cÃ¢u tÃ³m táº¯t dá»±a trÃªn Ä‘á»™ dÃ i ---
    if num_summary_sentences is None:
        total_sentences = len(cleaned_sentences)
        if total_sentences <= 5:
            num_summary_sentences = 1
        elif total_sentences <= 10:
            num_summary_sentences = 2
        elif total_sentences <= 20:
            num_summary_sentences = 3
        else:
            num_summary_sentences = min(5, total_sentences // 4)

    sentence_vectors = encode_sentences(cleaned_sentences)
    _, _, silhouette_scores = find_optimal_clusters(sentence_vectors)
    num_clusters = silhouette_scores.index(max(silhouette_scores)) + 2 if silhouette_scores else 1
    kmeans = cluster_sentences(sentence_vectors, num_clusters)
    clustered_summary = summarize_by_clustering(sentences, kmeans)

    # TÃ³m táº¯t heuristic tá»« káº¿t quáº£ phÃ¢n cá»¥m
    heuristic_sentences = tokenize_sentences(clustered_summary)
    cleaned = remove_stopwords(heuristic_sentences)
    tfidf_matrix = compute_tfidf(cleaned)
    cosine_sim = compute_cosine_similarity(tfidf_matrix)
    selected = heuristic_search(heuristic_sentences, cosine_sim, num_summary_sentences)

    # ÄÆ°a vÃ o mÃ´ hÃ¬nh há»c sÃ¢u Ä‘á»ƒ sinh báº£n tÃ³m táº¯t cuá»‘i cÃ¹ng
    return summarize_with_t5(selected, model, tokenizer)

# --- Giao diá»‡n Streamlit ---
def main():
    st.title("ðŸ“„ TÃ³m táº¯t vÄƒn báº£n báº±ng AI")
    st.write("TÃ­ch há»£p Heuristic + PhÃ¢n cá»¥m + MÃ´ hÃ¬nh há»c sÃ¢u Ä‘á»ƒ tÃ³m táº¯t tá»‘i Æ°u")

    text_input = st.text_area("âœï¸ Nháº­p Ä‘oáº¡n vÄƒn báº£n cáº§n tÃ³m táº¯t:", height=200)

    if st.button("ðŸš€ TÃ³m táº¯t"):
        if not text_input.strip():
            st.warning("Vui lÃ²ng nháº­p Ä‘oáº¡n vÄƒn báº£n.")
            return

        with st.spinner("â³ Äang tÃ³m táº¯t vÄƒn báº£n..."):
            # Hiá»ƒn thá»‹ thanh tiáº¿n trÃ¬nh
            progress_bar = st.progress(0)
            
            # Load model
            progress_bar.progress(25)
            model, tokenizer = load_finetuned_model()
            
            # Xá»­ lÃ½ vÄƒn báº£n
            progress_bar.progress(50)
            sentences = tokenize_sentences(text_input)
            cleaned_sentences = remove_stopwords(sentences)
            
            # PhÃ¢n cá»¥m vÃ  tÃ¬m cÃ¢u Ä‘áº¡i diá»‡n
            progress_bar.progress(75)
            
            # TÃ³m táº¯t cuá»‘i cÃ¹ng
            summary = hybrid_summarization(text_input, model, tokenizer)
            progress_bar.progress(100)

        st.subheader("ðŸ“ Báº£n tÃ³m táº¯t:")
        st.write(summary)

if __name__ == "__main__":
    main()
